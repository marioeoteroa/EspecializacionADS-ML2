{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "am0I4y6t4gtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 1\n",
        "\n",
        "1. Research about the Spectral Clustering method, and answer the following questions:\n",
        "a. In which cases might it be more useful to apply?\n",
        "b. What are the mathematical fundamentals of it?\n",
        "c. What is the algorithm to compute it?\n",
        "d. Does it hold any relation to some of the concepts previously mentioned in class? Which, and how?"
      ],
      "metadata": {
        "id": "O-nae94BsJhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectral clustering is a popular clustering algorithm used in machine learning and data analysis. Let's address your questions one by one:\n",
        "\n",
        "a. **In which cases might it be more useful to apply?**\n",
        "   Spectral clustering is particularly useful in cases where the data is not linearly separable, and traditional clustering methods like k-means may struggle. It is effective when dealing with non-convex clusters or when clusters have complex shapes. Spectral clustering can handle a wide range of data types, including graphs, images, and high-dimensional data. It is commonly used in image segmentation, community detection in social networks, and various other applications.\n",
        "\n",
        "b. **What are the mathematical fundamentals of it?**\n",
        "   Spectral clustering is based on spectral graph theory and linear algebra. The key mathematical concepts include:\n",
        "   - **Graph Theory**: Data is represented as a graph, where data points are nodes, and the edges between them represent pairwise relationships or similarities. The graph is typically represented as an adjacency matrix or a similarity matrix.\n",
        "   - **Laplacian Matrix**: The Laplacian matrix of the graph is a crucial component in spectral clustering. It is computed from the adjacency matrix and reflects the graph's structure.\n",
        "   - **Eigenvalues and Eigenvectors**: The Laplacian matrix is used to compute its eigenvalues and eigenvectors. These eigenvalues and eigenvectors hold essential information about the data's underlying structure.\n",
        "   - **Dimensionality Reduction**: Spectral clustering involves reducing the dimensionality of the data by selecting a few eigenvectors corresponding to the smallest eigenvalues.\n",
        "\n",
        "c. **What is the algorithm to compute it?**\n",
        "   The spectral clustering algorithm can be summarized as follows:\n",
        "\n",
        "   1. Construct an affinity matrix (similarity matrix) based on pairwise similarities or distances between data points.\n",
        "\n",
        "   2. Compute the Laplacian matrix from the affinity matrix. The most common Laplacian matrix used in spectral clustering is the unnormalized Laplacian.\n",
        "\n",
        "   3. Calculate the eigenvalues and eigenvectors of the Laplacian matrix.\n",
        "\n",
        "   4. Select a fixed number of eigenvectors corresponding to the smallest eigenvalues (typically, the first k eigenvectors, where k is the number of clusters you want to identify).\n",
        "\n",
        "   5. Form a new data matrix using the selected eigenvectors as columns.\n",
        "\n",
        "   6. Apply a standard clustering algorithm (e.g., k-means) to this new data matrix.\n",
        "\n",
        "d. **Does it hold any relation to some of the concepts previously mentioned in class? Which, and how?**\n",
        "   Spectral clustering is related to several concepts in machine learning and mathematics, including:\n",
        "\n",
        "   - **Graph Theory**: Spectral clustering relies heavily on graph theory concepts, such as adjacency matrices and Laplacian matrices.\n",
        "\n",
        "   - **Linear Algebra**: The computation of eigenvalues and eigenvectors of the Laplacian matrix involves linear algebra techniques.\n",
        "\n",
        "   - **Dimensionality Reduction**: Similar to Principal Component Analysis (PCA), spectral clustering involves reducing the dimensionality of data by selecting a subset of eigenvectors.\n",
        "\n",
        "   - **Clustering Algorithms**: Spectral clustering can be seen as a framework that combines spectral graph analysis with traditional clustering algorithms like k-means.\n",
        "\n",
        "   - **Kernel Methods**: Spectral clustering can be seen as a kernelized clustering method, as it can operate on data in high-dimensional spaces using kernel functions.\n",
        "\n",
        "In summary, spectral clustering is a powerful technique that leverages concepts from graph theory, linear algebra, and dimensionality reduction to identify complex clusters in data, making it a valuable tool in various machine learning and data analysis applications."
      ],
      "metadata": {
        "id": "PmYco8RwsQgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 2\n",
        "\n",
        "2. Research about the DBSCAN method, and answer the following questions:\n",
        "a. In which cases might it be more useful to apply?\n",
        "b. What are the mathematical fundamentals of it?\n",
        "c. Is there any relation between DBSCAN and Spectral Clustering? If so, what is it?\n"
      ],
      "metadata": {
        "id": "XQhwvOuEsJk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm widely used in data mining and machine learning. Let's address your questions about DBSCAN:\n",
        "\n",
        "a. **In which cases might it be more useful to apply?**\n",
        "   DBSCAN is particularly useful in cases where clusters have varying shapes, sizes, and densities. It excels at finding clusters in datasets with noise, where traditional distance-based methods like k-means may struggle. Some common scenarios where DBSCAN is useful include:\n",
        "   - Anomaly detection: Identifying data points that do not belong to any cluster (outliers).\n",
        "   - Spatial data analysis: Clustering geographic or location-based data where clusters may have irregular shapes.\n",
        "   - Image segmentation: Segmenting objects in images based on pixel similarities.\n",
        "\n",
        "b. **What are the mathematical fundamentals of it?**\n",
        "   The key mathematical concepts of DBSCAN include:\n",
        "   - **Density Estimation**: DBSCAN defines clusters based on the density of data points. It distinguishes between \"core points,\" \"border points,\" and \"noise points\" based on a user-defined distance threshold (epsilon) and a minimum number of points (MinPts).\n",
        "   - **Core Points**: These are data points within an epsilon radius of at least MinPts other data points.\n",
        "   - **Border Points**: These are data points within an epsilon radius of a core point but do not meet the MinPts criterion themselves.\n",
        "   - **Noise Points**: These are data points that are neither core points nor border points and are often considered outliers.\n",
        "   - **Connected Components**: DBSCAN connects core points and border points within the epsilon neighborhood to form clusters.\n",
        "\n",
        "c. **Is there any relation between DBSCAN and Spectral Clustering?**\n",
        "   DBSCAN and Spectral Clustering are both clustering algorithms, but they have different underlying principles and are suited to different types of data.\n",
        "\n",
        "   - **Density vs. Spectral Approach**: DBSCAN is density-based, meaning it defines clusters based on the density of data points. It identifies regions of high data point density as clusters. In contrast, Spectral Clustering is based on spectral graph theory and focuses on partitioning the graph representation of data into clusters.\n",
        "\n",
        "   - **Handling Noisy Data**: DBSCAN is particularly good at handling noisy data and identifying outliers as noise points. Spectral Clustering does not explicitly identify noise points but can be combined with techniques to handle noise.\n",
        "\n",
        "   - **Cluster Shape**: DBSCAN can identify clusters of arbitrary shapes, while Spectral Clustering assumes that clusters are defined by connected components in a graph, which may not be suitable for all types of data.\n",
        "\n",
        "   In summary, DBSCAN and Spectral Clustering serve different clustering needs. DBSCAN is ideal for identifying clusters with varying shapes and densities in noisy datasets, while Spectral Clustering is better suited for capturing complex relationships in data that can be represented as a graph."
      ],
      "metadata": {
        "id": "P-r2Z0kpsQ4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 3\n",
        "\n",
        "3. What is the elbow method in clustering? And which flaws does it pose to assess quality?"
      ],
      "metadata": {
        "id": "FN469cHisJpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow method is a popular technique used to determine the optimal number of clusters in a dataset for clustering algorithms like k-means. It helps find a suitable value for the parameter k, which represents the number of clusters. Here's how the elbow method works:\n",
        "\n",
        "1. **Run the Clustering Algorithm**: First, you perform the clustering algorithm (e.g., k-means) on the dataset for a range of values of k, typically from a small number to a reasonably large number. For each value of k, the algorithm assigns data points to clusters.\n",
        "\n",
        "2. **Calculate the Within-Cluster Sum of Squares (WCSS)**: For each value of k, calculate the sum of squared distances (SSD) between data points and their assigned cluster centroids. This metric is known as the Within-Cluster Sum of Squares. Mathematically, for a cluster i, WCSS can be calculated as:\n",
        "\n",
        "   \\[WCSS(k) = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} ||x_{ij} - c_i||^2\\]\n",
        "\n",
        "   where:\n",
        "   - \\(k\\) is the number of clusters.\n",
        "   - \\(n_i\\) is the number of data points in cluster \\(i\\).\n",
        "   - \\(x_{ij}\\) is a data point in cluster \\(i\\).\n",
        "   - \\(c_i\\) is the centroid of cluster \\(i\\).\n",
        "\n",
        "3. **Plot the Elbow Curve**: Plot the WCSS values against the corresponding values of k. The plot typically looks like an \"elbow,\" where the WCSS decreases as k increases. The idea is to find the point on the curve where the decrease in WCSS begins to slow down significantly, forming an elbow-like bend.\n",
        "\n",
        "4. **Select the Elbow Point**: The value of k at the \"elbow\" of the curve is often considered the optimal number of clusters because it indicates a balance between minimizing WCSS (intra-cluster distance) and avoiding too many clusters (overfitting).\n",
        "\n",
        "**Flaws of the Elbow Method:**\n",
        "\n",
        "While the elbow method is a useful heuristic for selecting the number of clusters, it has some limitations and flaws:\n",
        "\n",
        "1. **Subjectivity**: The choice of the \"elbow point\" can be somewhat subjective, as there may not always be a clear and distinct elbow. The method relies on visual inspection, which can be ambiguous, especially in datasets with complex structures.\n",
        "\n",
        "2. **Dependence on Initialization**: The outcome of the elbow method can be influenced by the initial configuration of the centroids in the k-means algorithm. Different initializations may lead to different elbow points.\n",
        "\n",
        "3. **Doesn't Always Work Well**: The elbow method may not work effectively for all types of datasets, especially when clusters have irregular shapes, different sizes, or varying densities. In such cases, the elbow may not be clearly discernible.\n",
        "\n",
        "4. **May Not Capture Complex Structures**: The elbow method assumes that clusters have spherical shapes and uniform sizes. It may not be suitable for datasets with non-convex clusters or complex structures.\n",
        "\n",
        "In summary, while the elbow method is a valuable tool for determining the number of clusters in k-means and similar algorithms, it should be used with caution and in conjunction with other techniques, especially when dealing with complex and diverse datasets."
      ],
      "metadata": {
        "id": "WqnoEPu_uAZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 4\n",
        "\n",
        "4. Remember the unsupervised Python package you created in the previous unit? 😀It’s time for an upgrade.\n",
        "\n",
        "a. Implement the k-means module using Python and Numpy\n",
        "\n",
        "b. Implement the k-medoids module using Python and Numpy\n",
        "\n",
        "c. Remember to keep consistency with Scikit-Learn API as high as possible"
      ],
      "metadata": {
        "id": "nx9s-lcZsJrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 5\n",
        "\n",
        "5. Let’s use the newly created modules in unsupervised to cluster some toy data.\n",
        "\n",
        "a. Use the following code snippet to create scattered data X\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(\n",
        "n_samples=500,\n",
        "n_features=2,\n",
        "centers=4,\n",
        "cluster_std=1,\n",
        "center_box=(-10.0, 10.0),\n",
        "shuffle=True,\n",
        "random_state=1,\n",
        ")\n",
        "\n",
        "b. Plot the resulting dataset. How many clusters are there? How far are they from one another?\n",
        "\n",
        "c. For both k-means and k-medoids (your implementations), calculate the silhouette plots and coefficients for each run, iterating K from 1 to 5 clusters.\n",
        "\n",
        "d. What number of K got the best silhouette score? What can you say about the figures? Is this the\n",
        "expected result?\n"
      ],
      "metadata": {
        "id": "6L_Lr76isJub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X, y = make_blobs(\n",
        "    n_samples=500,\n",
        "    n_features=2,\n",
        "    centers=4,\n",
        "    cluster_std=1,\n",
        "    center_box=(-10.0, 10.0),\n",
        "    shuffle=True,\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Scattered Data')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5ZbL1Lwbt6Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, KMedoids\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import numpy as np\n",
        "\n",
        "K_range = range(1, 6)\n",
        "\n",
        "for K in K_range:\n",
        "    kmeans = KMeans(n_clusters=K, random_state=1)\n",
        "    kmedoids = KMedoids(n_clusters=K, random_state=1)\n",
        "\n",
        "    kmeans_labels = kmeans.fit_predict(X)\n",
        "    kmedoids_labels = kmedoids.fit_predict(X)\n",
        "\n",
        "    kmeans_silhouette_avg = silhouette_score(X, kmeans_labels)\n",
        "    kmedoids_silhouette_avg = silhouette_score(X, kmedoids_labels)\n",
        "\n",
        "    print(f'K = {K}')\n",
        "    print(f'K-Means Silhouette Score: {kmeans_silhouette_avg:.2f}')\n",
        "    print(f'K-Medoids Silhouette Score: {kmedoids_silhouette_avg:.2f}')\n",
        "    print('------------------')\n"
      ],
      "metadata": {
        "id": "c2QSUr4eys0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dYbddSmZsJzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 6\n",
        "\n",
        "6. Use the following code snippet to create different types of scattered data:\n",
        "import numpy as np\n",
        "from sklearn import cluster, datasets, mixture\n",
        "### ============\n",
        "### Generate datasets. We choose the size big enough to see the scalability\n",
        "### of the algorithms, but not too big to avoid too long running times\n",
        "### ============\n",
        "n_samples = 500\n",
        "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
        "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
        "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
        "no_structure = np.random.rand(n_samples, 2), None\n",
        "### Anisotropically distributed data\n",
        "random_state = 170\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(X, transformation)\n",
        "aniso = (X_aniso, y)\n",
        "### blobs with varied variances\n",
        "varied = datasets.make_blobs(\n",
        "n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
        ")\n",
        "\n",
        "a. Plot the different datasets in separate figures. What can you say about them?\n",
        "\n",
        "b. Apply k-means, k-medoids, DBSCAN and Spectral Clustering from Scikit-Learn over each dataset and compare the results of each algorithm with respect to each dataset.\n"
      ],
      "metadata": {
        "id": "P7A6y_4LsJw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import cluster, datasets, mixture\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate datasets\n",
        "n_samples = 500\n",
        "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
        "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
        "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
        "no_structure = (np.random.rand(n_samples, 2), None)\n",
        "\n",
        "random_state = 170\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(X, transformation)\n",
        "aniso = (X_aniso, y)\n",
        "\n",
        "varied = datasets.make_blobs(\n",
        "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
        ")\n",
        "\n",
        "# Create a list of datasets\n",
        "datasets_list = [\n",
        "    noisy_circles,\n",
        "    noisy_moons,\n",
        "    blobs,\n",
        "    no_structure,\n",
        "    aniso,\n",
        "    varied,\n",
        "]\n",
        "\n",
        "# Plot the different datasets\n",
        "for i, dataset in enumerate(datasets_list, start=1):\n",
        "    X, _ = dataset\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(X[:, 0], X[:, 1], s=10)\n",
        "    plt.title(f'Dataset {i}')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "# Apply clustering algorithms to each dataset and compare results\n",
        "clustering_algorithms = {\n",
        "    'K-Means': cluster.KMeans(n_clusters=3),\n",
        "    'K-Medoids': cluster.KMedoids(n_clusters=3),\n",
        "    'DBSCAN': cluster.DBSCAN(eps=0.3, min_samples=5),\n",
        "    'Spectral Clustering': cluster.SpectralClustering(n_clusters=3),\n",
        "}\n",
        "\n",
        "for i, dataset in enumerate(datasets_list, start=1):\n",
        "    X, _ = dataset\n",
        "    for name, algorithm in clustering_algorithms.items():\n",
        "        algorithm.fit(X)\n",
        "        if hasattr(algorithm, 'labels_'):\n",
        "            y_pred = algorithm.labels_.astype(np.int)\n",
        "        else:\n",
        "            y_pred = algorithm.predict(X)\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=10)\n",
        "        plt.title(f'{name} - Dataset {i}')\n",
        "        plt.xlabel('Feature 1')\n",
        "        plt.ylabel('Feature 2')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "Aa49FsN-u4S7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}