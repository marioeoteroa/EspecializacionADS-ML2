{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fecha: 20/09/2023\n",
        "\n",
        "Realizado por: Mario Earles Otero Andrade\n",
        "\n",
        "Materia: Machine Learning II"
      ],
      "metadata": {
        "id": "_P8oA6O_CJ81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 1\n",
        "\n",
        "1. In your own words, describe what vector embeddings are and what they are useful for."
      ],
      "metadata": {
        "id": "qTL0xyX0Cwgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector embeddings are numerical vector representations of objects, such as words or phrases, derived through machine learning models. They capture semantic relationships and are useful in various natural language processing tasks including similarity estimation, clustering, and classification. They effectively facilitate the mathematical manipulation of linguistic concepts, allowing computers to 'understand' textual data in a more nuanced manner."
      ],
      "metadata": {
        "id": "g9EMWzAMEX_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 2\n",
        "\n",
        "2. What do you think is the best distance criterion to estimate how far two embeddings (vectors) are from each other? Why?\n"
      ],
      "metadata": {
        "id": "LzD1in0WCwja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most used and effective criterion tends to be cosine similarity to determine how close two embeddings are to each other, as it focuses on the angle between two vectors, disregarding their magnitude. This is especially useful in high-dimensional spaces, common in natural language processing tasks, as it can capture semantic similarity more precisely than other distance metrics. However, the \"best\" metric can depend on the specific context and the characteristics of the data."
      ],
      "metadata": {
        "id": "HF5EL7prEg1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 3\n",
        "\n",
        "3. Let us build a Q&A (question answering) system!😀For this, consider the following steps:\n",
        "\n",
        "a. Pick whatever text you like, in the order of 20+ paragraphs.\n",
        "\n",
        "b. Split that text into meaningful chunks/pieces.\n",
        "\n",
        "c. Implement the embedding generation logic. Which tools and approaches would help you generate them easily and high-level?\n",
        "\n",
        "d. For every question asked by the user, return a sorted list of the N chunks/pieces in your text that relate the most to the question. Do results make sense?"
      ],
      "metadata": {
        "id": "H1ZSpl3_CwmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exercise using a Hugging Face model, specifically using distilbert-base-multilingual-cased, which works well with multiple languages, including English.\n"
      ],
      "metadata": {
        "id": "70enA_7JCwux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the necessary libraries\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T44dH4gBipsZ",
        "outputId": "27a17722-d19e-4e5e-e591-bdb5e98adc12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries and prepare the text\n",
        "import torch\n",
        "import re\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n"
      ],
      "metadata": {
        "id": "E0armBb7ioD5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to split the text into chunks\n",
        "def split_into_paragraphs(text):\n",
        "    return [p for p in re.split(r'\\n+', text) if p]\n",
        "\n",
        "def split_into_n_words(text, n=50):\n",
        "    words = text.split()\n",
        "    chunks = [' '.join(words[i:i+n]) for i in range(0, len(words), n)]\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ZILIyyyumFsB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the text into embeddings\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "    return output.last_hidden_state.mean(dim=1)"
      ],
      "metadata": {
        "id": "DqnRliVwirlx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Given a question, we compute its embedding and then compute the cosine similarity with each paragraph's embedding\n",
        "def get_relevant_chunks(question, embeddings, chunks):\n",
        "    question_embedding = get_embedding(question)\n",
        "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    scores = [(cos(question_embedding, emb), chunk) for emb, chunk in zip(embeddings, chunks)]\n",
        "    scores.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [entry[1] for entry in scores]"
      ],
      "metadata": {
        "id": "BM6oTskuitWR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your complete text\n",
        "text_input = \"\"\"\n",
        "Los términos Analytics y Big Data son cada vez más socorridos y demandados en el entorno profesional a pesar de ser pocos los que verdaderamente conocen los principios que los sustentan. En este artículo nos adentramos en lo que se ha dado en denominar Big Data en Ingles, o lo que podemos traducir al español como la ciencia que permite extraer valor de grandes cantidades de datos. Algo que está revolucionando la forma en que se toman las decisiones que nos afectan en nuestro día a día. A continuación, detallaremos aspectos históricos, éticos y tecnológicos; analizando bondades y riesgos que acompañaremos con casos prácticos reales.\n",
        "¿Qué es la Ciencia de Datos? Se trata de una disciplina que permite tomar decisiones informadas y soportadas en evidencias científicas combinando: Experiencia de negocio, para detectar necesidades y definir casos de uso; Conocimientos matemáticos, para dar respuestas fundamentadas; e Ingeniería Informática, para escalar los cálculos y realizarlos de forma optimizada.\n",
        "¿Cómo es el proceso analítico? Como hemos dicho, todo empieza por tener datos primando siempre calidad frente al volumen. Una vez que tenemos esos datos, gracias a la matemática y la estadística, podemos analizar tendencias y relaciones entre los mismos para extrapolar conclusiones. En términos sencillos, buscar patrones que se repiten entre ellos.\n",
        "Una vez hecho este análisis, seremos capaces, mediante otras técnicas, de algo que solo los ordenadores nos permiten hoy en día, crear modelos. Un modelo es una representación / simulación de la realidad en la que entran unos datos y salen otros. Estos modelos pueden ser de diversa índole, pero en términos generales hablamos de: predicciones, clasificaciones o recomendaciones.\n",
        "Por hacerlo más claro, ya desde hace años un experto en una materia mediante observación y registro sencillo de datos, muchas veces en papel, era capaz de acertar con una precisión impecable una determinada avería. Un ejemplo claro lo tenéis en un mecánico de un coche. Esto es gracias a que el cerebro humano también analiza lo que ve y busca patrones y es capaz de inferir relaciones de correlación y/o causalidad. Note el lector más atento que correlación no implica causalidad.\n",
        "Lo que la Ciencia de Datos nos ha brindado es hacerlo: de una forma más precisa, manejando muchos más datos, evaluando muchos más patrones sin olvidar nada y no siendo dependiente del experto.\n",
        "Una vez cargado el modelo en un ordenador, podemos automatizar el proceso de análisis y la obtención de resultados. Todo ello para cualquier proceso que podamos imaginar. Es posible dar otro paso más, tratando de acercar la inteligencia artificial al modo en que los humanos tomamos decisiones.\n",
        "Dicho de otra forma, este modelo puede evolucionar a medida que trabaja, lee datos, produce una salida y se alimenta de los resultados reales auto ajustándose (aprendiendo) sobre la marcha. Esto es parte de lo que se denomina aprendizaje de las maquinas o 'Machine Learning' y su desarrollo y generalización ha sido posible gracias a la disponibilidad de grandes cantidades de datos, el abaratamiento de los costes, el incremento de la capacidad de procesado y la generalización del acceso a las técnicas y algoritmos\n",
        "¿Cómo ha evolucionado la Ciencia de Datos? Tras la II Guerra Mundial el taylorismo se adopta de manera generalizada a la par que los ordenadores se investigan extensamente. En 1962 John W. Tukey estadístico y considerado el padre de la Ciencia de Datos se cuestiona el futuro de la estadística como ciencia empírica. A principios de los 70 Peter Drucker, consultor de negocio, introduce el término 'Trabajador del conocimiento' y en paralelo Peter Naur acuño el término que utilizamos actualmente como Data Science (Ciencia de Datos).\n",
        "Ya para los 80 el uso de Microsoft Excel se generaliza como la herramienta base de los Trabajadores del conocimiento y a finales de los 90 internet conecta el mundo y pone a disposición de todos el acceso al conocimiento. Este primer internet también se conoce como internet 1.0 o del contenido. Seguidamente, en los 2000, surge el boom de las DOTCom. Todas las empresas ponen su negocio online. Se empieza a gestar el internet 2.0 o de los servicios (tiendas online, acceso a plataformas web, contratación de servicios). Por aquel entonces -principios de los 2000- William S. Cleveland, informático y estadístico, introduce la Ciencia de Datos como una disciplina unificada e independiente.\n",
        "Desde el 2005 en adelante las empresas de servicios comienzan a transformar la experiencia de usuario basada en datos y analítica. Las redes sociales toman peso y surge lo que conocemos como el internet 3.0 o de las personas donde lo que prima es estar conectados. El uso extensivo y generalizado de los smartphones da lugar al comienzo de la era de economía bajo demanda, surgen empresas como Uber, Airbnb, Amazon, Netflix (vertical de streaming) entre otras. Los dispositivos que utilizamos en el día a día como electrodomésticos y coches están cada vez más interconectados en lo que se ha denominado el internet 4.0 o de las cosas (IoT o Internet of Things).\n",
        "La previsión es que se extienda el uso de Machine Learning e Inteligencia Artificial a cada vez más ámbitos gracias a la tecnificación de usuarios y especialistas; y la generalización, aumento y abaratamiento de la capacidad de cálculo.\n",
        "¿Qué hay de verdad en la Ciencia de Datos? Este es uno de los casos donde la realidad supera a la ficción. La Ciencia de Datos tiene una particularidad que explica su crecimiento y es que es una disciplina muy accesible, pues el conocimiento y la tecnología están disponibles de forma generalizada y a muy bajo coste.\n",
        "Hace unos meses comentaba Enrique Tellado en una mesa redonda algo, a priori, utópico: la posibilidad de vivir en una sociedad donde, gracias a la tecnología, el trabajo tal y como lo entendemos en jornadas de 8 horas diarias, no fuera necesario. Dejando a un lado los matices políticos, esta es una utopía que podríamos ver materializada a finales de este siglo. Esto por no hablar por los avances en el mundo médico y en otras disciplinas. Como defiende Steven Pinker en su obra 'Los Ángeles que Llevamos Dentro', el progreso como sociedad es innegable, cada vez vivimos mejor. En esto la tecnología, la ciencia de datos y la inteligencia artificial están jugando un papel clave.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RV59Aufwma1B"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose how to split the text: by paragraphs or by N words\n",
        "chunks = split_into_paragraphs(text_input)  # Or use split_into_n_words(text_input, N)\n",
        "#chunks = split_into_n_words(text_input, 50)\n",
        "# Generate embeddings for each chunk\n",
        "embeddings = [get_embedding(chunk) for chunk in chunks]"
      ],
      "metadata": {
        "id": "kP4R_44TmZH5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the most relevant chunks based on a question\n",
        "question = \"Que es la ciencia de datos?\"\n",
        "relevant_chunks = get_relevant_chunks(question, embeddings, chunks)\n",
        "print(relevant_chunks[0])  # This will give you the most relevant chunk.\n",
        "#print(relevant_chunks[1])\n",
        "#print(relevant_chunks[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G974qRbmyA5",
        "outputId": "68b85c80-3930-4dd6-c1c4-041e43f69e31"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Qué hay de verdad en la Ciencia de Datos? Este es uno de los casos donde la realidad supera a la ficción. La Ciencia de Datos tiene una particularidad que explica su crecimiento y es que es una disciplina muy accesible, pues el conocimiento y la tecnología están disponibles de forma generalizada y a muy bajo coste.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 4\n",
        "\n",
        "4. What do you think that could make these types of systems more robust in terms of semantics and functionality?\n"
      ],
      "metadata": {
        "id": "y9HDBS2ECwxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improving the robustness of question-answering systems in terms of semantics and functionality is an active area of research. Here are several strategies that could make these systems more robust:\n",
        "\n",
        "1. **Better Pre-training on Diverse Data**:\n",
        "   - Models like BERT, GPT, and their variants are pre-trained on large datasets. However, training them on even more diverse datasets, including data from various domains, languages, and cultures, can help them understand semantics better.\n",
        "  \n",
        "2. **Incorporate External Knowledge Bases**:\n",
        "   - Integrating information from structured knowledge bases like Wikipedia, DBpedia, or the Semantic Web can enhance the system's understanding. This integration helps in grounding the textual data with real-world facts and entities.\n",
        "   \n",
        "3. **Fine-tuning on Domain-specific Data**:\n",
        "   - For a specific application or domain, the model can be fine-tuned on relevant datasets to ensure it captures the domain's nuances and semantics.\n",
        "\n",
        "4. **Improving Contextual Understanding**:\n",
        "   - Using models that can consider broader contexts or longer sequences of text can be beneficial. For instance, models like Transformer-XL and Longformer are designed to handle longer contexts.\n",
        "\n",
        "5. **Multimodal Models**:\n",
        "   - Incorporating other forms of data, like images or sounds, can provide additional context. Models like CLIP and ViLBERT, which consider both text and images, can offer richer semantic understanding.\n",
        "\n",
        "6. **Attention Visualization and Analysis**:\n",
        "   - Visualizing where the model is \"looking\" in the input (using attention weights) can provide insights into its decision-making process, helping to identify areas of improvement.\n",
        "\n",
        "7. **Regularization and Adversarial Training**:\n",
        "   - Introducing noise or adversarial examples during training can make the model more robust to slight alterations in the input, ensuring consistent responses.\n",
        "\n",
        "8. **Feedback Loops**:\n",
        "   - Systems that can learn continuously from user feedback or corrections will be better aligned with human expectations over time.\n",
        "\n",
        "9. **Better Handling of Ambiguity**:\n",
        "   - Instead of providing a single answer, the model can be designed to return a list of possible answers or ask clarifying questions when faced with ambiguous queries.\n",
        "\n",
        "10. **Ethical and Bias Considerations**:\n",
        "   - Regularly auditing and fine-tuning the models to reduce biases will make them more trustworthy and semantically accurate. Using tools and datasets explicitly designed for bias detection and mitigation can be beneficial.\n",
        "\n",
        "11. **Incorporate Symbolic Reasoning**:\n",
        "   - Hybrid models that combine deep learning with symbolic reasoning can perform logical deductions, ensuring that the system doesn't just rely on pattern recognition but also on logical rules.\n",
        "\n",
        "12. **Continual Learning**:\n",
        "   - Models that can learn over time without forgetting previously learned information can adapt to evolving languages and semantics.\n",
        "\n",
        "13. **Multilingual and Cross-lingual Models**:\n",
        "   - These models can transfer knowledge between languages, helping in understanding semantics in a more universal context.\n",
        "\n",
        "By combining these strategies, one can build a question-answering system that is not only functionally robust but also semantically accurate and aligned with human expectations."
      ],
      "metadata": {
        "id": "7xoeZeUBnYVB"
      }
    }
  ]
}